{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"float:left\"><h1> Recommender Systems </h1></div>\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:65px\" src =\"https://drive.google.com/uc?export=view&id=1EnB0x-fdqMp6I5iMoEBBEuxB_s7AmE2k\" />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zph3Xuf2MpCv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka2AUUdBMpCt"
   },
   "source": [
    "In this lecture we will focus on one of the most widely used data science applications, recommender systems. At a high level, these systems aim to predict how a user feels about various items. We will focus on specific applications of these predictions, develop our own basic systems and see how to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3jbrfSFMpC2"
   },
   "source": [
    "### What are Recommender Systems\n",
    "\n",
    "Generally a recommender system provides a user with a list of options it thinks they would like to try. You probably see these systems every day on websites like Amazon or Spotify. These companies want you to continually use their service and make purchases from them, their income is highly dependent on providing recommendations you follow up on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ba77Xy9LMpC5"
   },
   "source": [
    "<img src = \"https://drive.google.com/uc?export=view&id=1TxpAHfLkp4PkVerZ0dRF3bySthWYQeKD\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWp9-k3YMpC-"
   },
   "source": [
    "At its core, a recommender system contains information about many users and their ratings of certain items, and it tries to predict what kind of rating a user will give an item they have never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2GxrLAmMpC_"
   },
   "source": [
    "### Movie Rating System\n",
    "\n",
    "We're going to use a dataset of movies from the internet movie database, that is our system will recommend movies to a user. \n",
    "\n",
    "Let's load up some basic data to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tscVYsHxMpDD",
    "outputId": "1f316cae-437d-4aa4-c784-c600093f3d89"
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv('data/movies_metadata.csv')\n",
    "movie_df.drop_duplicates(subset=['title'], inplace=True)\n",
    "movie_df = movie_df.reset_index().drop('index', axis=1)\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_df[['id', 'title', 'overview', 'vote_average', 'vote_count']]\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bwywil4RMpDL"
   },
   "source": [
    "### A User Independent System\n",
    "\n",
    "We'll start with building recommendations for the case where we have no knowledge about a user. In this case, we probably would find the most popular items, and recommend them to our user.\n",
    "\n",
    "The simplest system would just sort the items based on their average score and present a list in that order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0zsn208MpDN",
    "outputId": "21eab0f6-0399-4073-ad71-da2ece3c384a"
   },
   "outputs": [],
   "source": [
    "top_rated = movie_df.sort_values(by=['vote_average'], ascending=False)\n",
    "top_rated['title'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tcNJekqMpDT"
   },
   "source": [
    "Those results seem a bit interesting... Let's look at the top few items in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQXuCQVzMpDU",
    "outputId": "8c07ac90-828b-4a85-d5a4-159b5beb9536"
   },
   "outputs": [],
   "source": [
    "top_rated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhwBmk7oMpDZ"
   },
   "source": [
    "There's a big issue with this system, the top ranked items have a perfect average score but only one review. Let's look at a movie which is generally considered highly rated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ot6ZG_a3MpDa",
    "outputId": "15d6af61-d1cd-4d06-e152-33b720bd7f45"
   },
   "outputs": [],
   "source": [
    "top_rated[top_rated['title'] == \"Citizen Kane\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvaf6SsLMpDf"
   },
   "source": [
    "Citizen Kane, which many consider the greatest movie ever made, only has a score of $8.0$ but it has over $1200$ reviews. The problem is a more esoteric movie which is only known of by a very narrow audience is likely to have a few extremely positive reviews. But this is probably something we wouldn't want to push to the top of a movie recommendation list!\n",
    "\n",
    "One way to deal with items with few reviews is to simply ignore them. This is known as thresholding, we set some threshold for required reviews, we simply ignore items with too few reviews. If we were to implement this with our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAxLhAt0MpDh",
    "outputId": "3fe4b3ed-eb79-4419-b9ef-9f506da6544e"
   },
   "outputs": [],
   "source": [
    "threshold = 1000\n",
    "movies_with_many_reviews_df = movie_df[movie_df['vote_count'] >= threshold]\n",
    "\n",
    "top_rated_v2 = movies_with_many_reviews_df.sort_values(by=['vote_average'], ascending=False)\n",
    "top_rated_v2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPNE2dw_MpDp"
   },
   "source": [
    "We see the recommendations have significantly changed. While this is a nice fix there are two major issues:\n",
    "\n",
    "1. We selected the threshold in an arbitrary fashion, it seems like a reasonable number, but we kind of just made it up.\n",
    "2. If an item must meet a threshold to appear in a recommended list but it never appears in a list not many people will watch it. That is it will have a hard time getting the reviews it needs. \n",
    "\n",
    "The first issue can be difficult to deal with, but we can perform user studies and see what is a reasonable threshold. For the second issue one possible fix is to occasionally ignore the threshold requirements for new items, this allows them to build up some initial reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-independent System Review\n",
    "\n",
    "When we have no information about a user or their preferences, we can just show them the most popular movies given some review threshold. For new movies, we can sometimes show them to users and ignore the thresholding.\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SwytJSxMpDr"
   },
   "source": [
    "### Content Based Recommendations\n",
    "\n",
    "What if we know something about our user? For example, we know they like Iron Man movies. Then we can employ content-based recommendations.\n",
    "\n",
    "These recommendation engines are built around the idea if a user likes some item (or a particular basket of items) then they will like similar items based on the item content/description. If I watched the Avengers, then I probably would want to watch other superhero-themed movies. If we look at a few movies along with their descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKdqLIr-MpDx",
    "outputId": "a9251970-4ddf-4543-c70c-2df48cc6a78f"
   },
   "outputs": [],
   "source": [
    "df_descriptions = movie_df[['title', 'overview']]\n",
    "df_descriptions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cia5BpvxMpEB"
   },
   "source": [
    "What does it mean for two items to be similar from a content perspective? \n",
    "\n",
    "Well first, we need to put the content into a format a model can understand. Right now we only have movie descriptions and we can't really feed this straight into some model. Instead we will use the Term Frequency Inverse Document Frequency measure (TF-IDF).\n",
    "\n",
    "\n",
    "The TF-IDF is made of two measures, term frequency (TF) and inverse document frequency (IDF). \n",
    "\n",
    "For a term (a word or phrase), $t$, and a document ,$d$, the term frequency $\\text{TF}(t,d)$, measures how common the term is in the document. \n",
    "\n",
    "The inverse document frequency of a term $t$, $\\text{IDF}(t)$, is the inverse of the number of documents a term $t$ appears in.\n",
    "\n",
    "The intuition is that when a term appears in many documents, it's not very relevant. However, if it only appears in a few documents, it's very relevant to identifying those documents.\n",
    "\n",
    "For a complete description see the [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) page but what you need to know is: \n",
    "\n",
    "- The more common a term is across all documents the more $\\text{IDF}(t)$ shrinks, \n",
    "- The more common a term is within one document the more $\\text{TF}(t,d)$ grows. \n",
    "- The term frequency inverse document frequency is simply the product of these two: \n",
    "    $$\\text{TF-IDF}(t,d) = \\text{TF}(t,d)\\times  \\text{IDF}(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhcTWIk-MpED"
   },
   "source": [
    "Now we can transform our data using the TF-IDF measure. Each movie is a document, or more precisely each description is a document, and for now let's keep it simple, terms will just be individual words. We can use built in python functionality to make our life easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\", min_df=2)\n",
    "movie_df['overview'] = movie_df['overview'].fillna(\"\")\n",
    "\n",
    "TF_IDF_matrix = vectorizer.fit_transform(movie_df['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42,728 movies by 36,504 tokens in the descriptions =~ 1.5B elements!\n",
    "TF_IDF_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6JcyYOjMpEn"
   },
   "source": [
    "The matrix returned is quite large, each row is a document and each column is a term. Entry ${(i,j)}$ is just the TF-IDF$(i,j)$.\n",
    "\n",
    "Now that we have our data in a numeric format how can we measure the similarity between two documents. Each document is a row in our sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_matrix[(movie_df['title'] == 'Citizen Kane').values].todense().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKySuos-MpFV"
   },
   "source": [
    "We see these are just numeric arrays, that is vectors. There are several common ways to compare two vectors $a$ and $b$, probably the most common in recommender systems is the cosine similarity:\n",
    "\n",
    "$$\\frac{a \\cdot b}{||a||\\cdot ||b||}$$\n",
    "\n",
    "The closer two vectors (or documents) are, the higher this measure. We can calculate the similarity using cosine similiarity using sklearn's cosine similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df[movie_df['title'].str.contains('Captain America', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reo1fBcVMpFk",
    "outputId": "9e41c007-066f-4ab0-c1c1-c779dd41cf08"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "movie_1 = TF_IDF_matrix[(movie_df['title'] == 'Captain America: The First Avenger').values,]\n",
    "movie_2 = TF_IDF_matrix[(movie_df['title'] == 'Captain America: The Winter Soldier').values,]\n",
    "\n",
    "print(\"Similarity:\", cosine_similarity(movie_1, movie_2)) # Notice the result is a 2D 1X1 array, so to grab\n",
    "                                                          # the number we will need to index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1JrCjYsMpF2"
   },
   "source": [
    "Not only can we use the `sklearn.metrics.pairwise.cosine_similiarity` function to compute that between two different vectors, we can pass the entire tf-idf matrix into the function as a single argument and it will compute the similarity between each column and every other column, giving back a square matrix, where the entry at $(i, j)$ is the similarity between movie $i$ and $j$ (like a correlation matrix for features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(TF_IDF_matrix, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "# rows and columns should be equal, and the number of movies we started with (rows)\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can directly compare two movies and we can make recommendations of the form: if you like movie $a$ then you will also like movies $b$, $c$, $d$, $etc$. \n",
    "\n",
    "We can do this just picking a candidate film and taking its column in the similarity matrix, and then finding those rows where the similarities are highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample movie\n",
    "movie_df[movie_df['title'] == 'Captain America: The Winter Soldier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column based upon the index\n",
    "movie_index = movie_df[movie_df['title'] == 'Captain America: The Winter Soldier'].index\n",
    "\n",
    "# Create a dataframe with the movie titles\n",
    "sim_df = pd.DataFrame({'movie':movie_df['title'], \n",
    "                       'similarity': np.array(similarities[movie_index, :].todense()).squeeze()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the top 10 most similar movies\n",
    "sim_df.sort_values(by='similarity', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's now implement this using a simple function, since we must first find the corresponding column in the similarity matrix. We'll also add a parameter for a vote threshold, to restrict our results to above a certain threshold of votes, so that we can only return more popular films if we'd like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NoQoisR_MpF7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def content_recommender(title, movies, similarities, vote_threshold=10) :\n",
    "    \n",
    "    # Get the movie by the title\n",
    "    movie_index = movies[movies['title'] == title].index\n",
    "    \n",
    "    # Create a dataframe with the movie titles\n",
    "    sim_df = pd.DataFrame(\n",
    "        {'movie': movies['title'], \n",
    "         'similarity': np.array(similarities[movie_index, :].todense()).squeeze(),\n",
    "         'vote_count': movies['vote_count']\n",
    "        })\n",
    "    \n",
    "    # Get the top 10 movies with > 10 votes\n",
    "    top_movies = sim_df[sim_df['vote_count'] > vote_threshold].sort_values(by='similarity', ascending=False).head(10)\n",
    "    \n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvytHz79MpGB",
    "outputId": "fe4f0c78-1dd0-4db7-dabf-8d322d94f2c8"
   },
   "outputs": [],
   "source": [
    "# Test the recommender\n",
    "similar_movies = content_recommender(\"Thor\", movie_df, similarities, vote_threshold=1000)\n",
    "similar_movies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Based System Review\n",
    "\n",
    "These systems try and recommend new items (in our case movies) based on the idea that if you like a certain movie, you will like movies that are similar in their description. We could also include more information than just the vectorized description text by adding other features, for example, whether films were of a particular genre or had a particular director or actor.\n",
    "\n",
    "This is a more advanced recommendation system than the user independent one, and certainly gives better recommendations. However we also need to be mindful about sometimes returning some variety in our results to make sure users don't get bored with our recommendations.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJXZ6p7NMpGf"
   },
   "source": [
    "### Collaborative Based Recommendations\n",
    "\n",
    "Collaborative filtering also relies on similarity between items, as well as similarity between users. However, it define similarity slightly differently.\n",
    "\n",
    "The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion/taste as a person B on an issue/item, A is more likely to have B's opinion on a different issue/item than that of a randomly chosen person.\n",
    "\n",
    "Collaborative methods for recommender systems are methods that are based solely on the past interactions recorded between users and items in order to produce new recommendations. These interactions are stored in the so-called “user-item interactions matrix”.\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1000/1*m_Z6Da5FZ62KN2yH-x_GOQ@2x.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Unlike content-based systems, a collaborative system looks at an item as a collection of ratings. Every item has ratings by some users, and if two items get very similar ratings from users, the items themselves are similar (notice this system is not at all aware of the items' content).\n",
    "\n",
    "\n",
    "\n",
    "Similarly, we can define users to be similar if they rate items similarly.\n",
    "\n",
    "\n",
    "To actually do collaborative filtering we'll need some more data, now we'll user data with movie reviews from various users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_vzdNK0MpGh",
    "outputId": "3605639b-7977-42e1-cb3c-7f689a6439ce"
   },
   "outputs": [],
   "source": [
    "columns = ['user', 'movie', 'score', 'timestamp']\n",
    "df = pd.read_csv('data/u.data', sep='\\t', names=columns).drop('timestamp',axis=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we only have the review for each user and movie combination. In essence, we know user 196 gave movie 242 a review of 3, but we know nothing of the movie content or the user preference.\n",
    "\n",
    "The first thing we need to do is turn the dataframe we have into a matrix $R$, where each entry in $R$, $R_{um}$ is the rating user $u$ gave item $m$. This is referred to as the _utility matrix_. \n",
    "\n",
    "In our case the items, are movies. Let's get our user-item utility matrix $R$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfxKBMX8MpGs",
    "outputId": "58cf9cb6-9420-4d59-bad6-00bba3e9a2c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = df['user'].unique()\n",
    "movies = df['movie'].unique()\n",
    "\n",
    "num_users = len(users)\n",
    "num_movies = len(movies)\n",
    "print('num unique users:', len(users))\n",
    "print('num unique movies:', len(movies))\n",
    "           \n",
    "# np.nan means they user has not reviewed the movie\n",
    "R = np.full((num_users, num_movies), np.nan)\n",
    "\n",
    "#Build the user-item matrix\n",
    "for row in df.itertuples(): # same as zip(df.index, df[\"user\"], df[\"movie\"], df[\"score\"])\n",
    "    user = row[1]\n",
    "    movie = row[2]\n",
    "    rating = row[3]\n",
    "    R[user-1, movie-1] = rating\n",
    "    \n",
    "R_df = pd.DataFrame(data=R, index=range(1, num_users+1), columns=range(1, num_movies+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.itertuples())[0] # Pandas special named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this matrix, the rows are the different users (so we have 943 different users) and the columns are the movies (so we have 1,682 different movies). \n",
    "\n",
    "As expected, most of the entries are empty since not every user has watched every film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yzWh3pjMpG1"
   },
   "source": [
    "### User-Item Filtering\n",
    "\n",
    "Our first collaborative filtering model will be **_user-item filtering_**. User-item filtering works on the logic users with similar preferences on some item have similar preferences over other items. If we wanted to know what $R_{ij}$ is (what we think user $i$ would give to item $j$) we would find users similar to $i$ and determine what rank they gave $j$, we can use this as an estimate for $R_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ga6IQg0MMpG3"
   },
   "source": [
    "### Similar Users\n",
    "\n",
    "What makes two users similar? The simplest way to measure if user $a$ and $b$ are similar is to look at the items both of them reviewed. For example consider the following two users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6WSsNkoMpG4",
    "outputId": "0202f2a6-9200-443a-ef32-d53ca911bca2"
   },
   "outputs": [],
   "source": [
    "R_df.loc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nbzxxrlMpG9",
    "outputId": "683470fe-e6be-4f69-dca4-a87546cbf12d"
   },
   "outputs": [],
   "source": [
    "R_df.loc[16, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhBZ-9iLMpHG"
   },
   "source": [
    "We can compare them directly on the first movie since they both reviewed each of them. But we can't compare them on the second and third movie since only the first user reviewed both of those.\n",
    "\n",
    "\n",
    "We can find all of the movies that they both reviewed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_1st_user_rated = ~R_df.loc[1, :].isna()\n",
    "movies_2nd_user_rated = ~R_df.loc[16, :].isna()\n",
    "\n",
    "movies_both_users_rated = movies_1st_user_rated & movies_2nd_user_rated\n",
    "print(movies_both_users_rated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzVL_tQNMpHK",
    "outputId": "207ec61d-c707-4bed-9473-88c5512e77cc"
   },
   "outputs": [],
   "source": [
    "print(\"Reviewers' scores:\")\n",
    "R_df.loc[[1, 16], movies_both_users_rated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhBrZRgRMpHT"
   },
   "source": [
    "This subset of items forms a vector over the same dimensions (items) for each user, thus we can take some measure of vector similarity to find user similarity. Let's again use the cosine similarity to find how close two users are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odzQIEVTMpHW",
    "outputId": "74922167-142d-4d84-a798-3b4e647f49e8"
   },
   "outputs": [],
   "source": [
    "print(\"Similarity:\", cosine_similarity(R_df.loc[1, movies_both_users_rated].values.reshape(1,-1), \n",
    "                                       R_df.loc[16, movies_both_users_rated].values.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhiUq3bfMpHd"
   },
   "source": [
    "### Making Predictions\n",
    "\n",
    "Now that we have a way of directly comparing users we can find how similar user $u_i$ is to every other user who reviewed item $m$ (if we implement this for user 16 and item 1) we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_user_similarity(user_1, user_2, R_df):\n",
    "    \n",
    "    # Define the mask which finds all movies they rated together\n",
    "    movies_1st_user_rated = ~R_df.loc[user_1, :].isna()\n",
    "    movies_2nd_user_rated = ~R_df.loc[user_2, :].isna()\n",
    "\n",
    "    movies_both_users_rated = movies_1st_user_rated & movies_2nd_user_rated\n",
    "\n",
    "    # Sum boolean to get the counts\n",
    "    number_of_movies_rated_together = movies_both_users_rated.sum()\n",
    "        \n",
    "    # Find the ratings of both users for movies they both watched\n",
    "    ratings_of_user1 = R_df.loc[user_1, movies_both_users_rated].values.reshape(1, -1)\n",
    "    ratings_of_user2 = R_df.loc[user_2, movies_both_users_rated].values.reshape(1, -1)\n",
    "    \n",
    "    # Finally, calculate the similarity between them\n",
    "    similarity = cosine_similarity(ratings_of_user1, ratings_of_user2)[0][0]\n",
    "    \n",
    "    return similarity, number_of_movies_rated_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_user = 16 # we will only do this to user 16 for demonstration purposes\n",
    "current_movie = 2\n",
    "similarities_to_user_16 = []\n",
    "ratings_given_to_movie_2 = []\n",
    "\n",
    "# Find only the users who rated movie 2 (rows)\n",
    "R_df2 = R_df[~R_df.iloc[:, 1].isna()].copy()\n",
    "\n",
    "for other_user in R_df2.index:\n",
    "    \n",
    "    similarity, number_of_movies_rated_together = find_user_similarity(current_user, other_user, R_df)\n",
    "    similarities_to_user_16.append(similarity)\n",
    "    ratings_given_to_movie_2.append(R_df.loc[other_user, current_movie])\n",
    "            \n",
    "# Finally, let's turn these into numpy arrays so life is easier\n",
    "similarities_to_user_16 = np.array(similarities_to_user_16)\n",
    "ratings_given_to_movie_2 = np.array(ratings_given_to_movie_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the expected rating user 16 will give movie 2. We do so by a weighted average; We grab the score user $v$ gave movie 2, and multiply it by the similarity between user 16 and user $i$. Finally we divide by the sum of similarities. We do this for all $C$ users we have found that rated movies user 16 has watched.\n",
    "\n",
    "This can all be expressed as:\n",
    "\n",
    "$$\\hat{R_{16, 2}} = \\frac{\\sum_{v=1}^C R_{v, 2} \\cdot \\text{user_similarity}(16,v)}{\\sum_{v=1}^C\\text{user_similarity}(16, v)}$$\n",
    "\n",
    "for our particular case, or for the general case of user $u$ and movie $i$\n",
    "\n",
    "$$\\hat{R_{um}} = \\frac{\\sum_{v=1}^C R_{vm} \\cdot \\text{user_similarity}(u, v)}{\\sum_{v=1}^C \\text{user_similarity}(u, v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rating = np.dot(ratings_given_to_movie_2, similarities_to_user_16)/np.sum(similarities_to_user_16)\n",
    "\n",
    "print(f'Predicted rating for movie 2 by user 16 is {round(predicted_rating, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzSwDlXqMpHt"
   },
   "source": [
    "Now to find the overall recommendation list for user 16 we need to calculate this score for every item they haven't rated yet then sort that list, quite the task!\n",
    "\n",
    "------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpT4E744MpHu"
   },
   "source": [
    "### Item-Item Filtering\n",
    "\n",
    "Our second collaborative filter model will be item-item filtering. In user-item filtering, we said if two users are similar, and one of them rated item $i$, the other user's rating of item $i$ will be similar.\n",
    "\n",
    "In **_item-item filtering_**, we say that if two items are similar, and a user ranked one of those items, that user's rating of the other item will be similar.\n",
    "\n",
    "In essence the logic is very similar, except instead of judging user similarity, we will look at item similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijoRKMEkMpHv"
   },
   "source": [
    "### Similar Items\n",
    "\n",
    "What makes two items similar? As with measuring if users are similar there are many ways of measuring item similarity. For our simple method to find the similarity of two items, $x$ and $y$, we look at every user who reviewed both items. For example let us consider the items in columns 15 and 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_who_ranked_item_15 = ~R_df.loc[:, 15].isna()\n",
    "users_who_ranked_item_18 = ~R_df.loc[:, 18].isna()\n",
    "\n",
    "users_who_ranked_both_items = users_who_ranked_item_15 & users_who_ranked_item_18\n",
    "print(users_who_ranked_both_items.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzVL_tQNMpHK",
    "outputId": "207ec61d-c707-4bed-9473-88c5512e77cc"
   },
   "outputs": [],
   "source": [
    "print(\"Reviewers' scores:\")\n",
    "R_df.loc[users_who_ranked_both_items, [15, 18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhBrZRgRMpHT"
   },
   "source": [
    "Now we can judge how similar the ratings between items 15 and 18 are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df.loc[users_who_ranked_both_items, 18].values.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odzQIEVTMpHW",
    "outputId": "74922167-142d-4d84-a798-3b4e647f49e8"
   },
   "outputs": [],
   "source": [
    "print(\"Similarity:\", cosine_similarity(R_df.loc[users_who_ranked_both_items, 15].values.reshape(1,-1), \n",
    "                                       R_df.loc[users_who_ranked_both_items, 18].values.reshape(1,-1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_TUa2qqMpH2"
   },
   "source": [
    "### Making Predictions \n",
    "\n",
    "Now that we can measure how similar two items are we can use this information to make a prediction. The idea behind item-item filtering is the review user $u$ gives to item $i$ ($R_{um}$) should be similar to the review $u$ gives to a similar item $n$ ($R_{un}$), if items $m$ and $n$ are similar. \n",
    "\n",
    "We can start with an idea similar to what we did with user-item predictions. We'll define a set of items similar to $m$ that user $u$ reviewed. We can take the average rating for each item in this list and use it as the value for $R_{um}$. This time $C$ is our number of total movies:\n",
    "\n",
    "$$\\hat{R_{um}} = \\frac{\\sum_{n=1}^C R_{un} \\cdot \\text{item_similarity}(m, n)}{\\sum_{n=1}^C \\text{item_similarity(m, n)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "1. Finish implementing the item-item similarity function, the function should return -1, 0 if no users rated both items (look at the previous similarity function), and the similarity of the two items and number of reviewers otherwise.\n",
    "\n",
    "    Use this function to predict the rating user 16 would give movie 2. How does this compare with the previous result (user-item)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_item_similarity(item_1, item_2, R_df):\n",
    "    \n",
    "    # Define the mask which finds users who rated both items\n",
    "    users_who_reviewed_1st_item = R_df.loc[:, item_1].notna()\n",
    "    users_who_reviewed_2nd_item = R_df.loc[:, item_2].notna()\n",
    "\n",
    "    users_who_reviewed_both_items = users_who_reviewed_1st_item & users_who_reviewed_2nd_item\n",
    "    \n",
    "    # Find how many users rated both items\n",
    "    number_of_users = users_who_reviewed_both_items.sum()\n",
    "    \n",
    "    # Find the ratings of both users for movies they both watched\n",
    "    ratings_of_item1 = R_df.loc[users_who_reviewed_both_items, item_1].values.reshape(1, -1)\n",
    "    ratings_of_item2 = R_df.loc[users_who_reviewed_both_items, item_2].values.reshape(1, -1)\n",
    "    \n",
    "    # Finally, calculate the similarity between them\n",
    "    similarity = cosine_similarity(ratings_of_item1, ratings_of_item2)[0][0]\n",
    "    \n",
    "    return similarity, number_of_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_user = 16 # we will only do this to user 16 for demonstration purposes\n",
    "current_movie = 2\n",
    "similarities_to_movie_2 = []\n",
    "ratings_given_by_user_16 = []\n",
    "\n",
    "# Find only the items rated by user 16 (columns)\n",
    "R_df3 = R_df.loc[:,~R_df.iloc[15,:].isna()].copy()\n",
    "\n",
    "for other_movie in R_df3.columns:\n",
    "    \n",
    "    similarity, number_of_user_rated = find_item_similarity(current_movie, \n",
    "                                                                other_movie, \n",
    "                                                                R_df)\n",
    "    # Append similarity and ratings\n",
    "    similarities_to_movie_2.append(similarity)\n",
    "    ratings_given_by_user_16.append(R_df.loc[current_user, other_movie])\n",
    "            \n",
    "# Finally, let's turn these into numpy arrays so life is easier\n",
    "similarities_to_movie_2 = np.array(similarities_to_movie_2)\n",
    "ratings_given_by_user_16 = np.array(ratings_given_by_user_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rating = np.dot(ratings_given_by_user_16, similarities_to_movie_2)/np.sum(similarities_to_movie_2)\n",
    "\n",
    "print(f'Predicted rating for movie 2 by user 16 is {round(predicted_rating, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqElYGLcMpH4"
   },
   "source": [
    "### Memory Based Filtering\n",
    "\n",
    "These two methods are instances of what are generally known as memory based filtering. While there is no hard definition, it is generally understood to mean methods which work by using the entire set of user and item reviews  calculating the relevant information from this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AacQdVgUMpIw"
   },
   "source": [
    "### Collaborative  System Review\n",
    "\n",
    "Collaborative filtering is a powerful family of methods which leverage the decisions made by groups of users to make predictions. These models form the basis of various real world system and they tend to perform well in practice.\n",
    "\n",
    "That being said, there are drawbacks to collaborative systems. One of the biggest issues is the size and sparsity of the data. Consider the user-item matrix in our previous example. This matrix is huge, and it's from a pretty small example. Imagine the Amazon user-item data set: there are over three-hundred million active users and almost half a billion products. There's no way we could run our algorithms on it very frequently (if ever).\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization Methods and Latent Features (Model Based CF)\n",
    "\n",
    "The last method we are going to discuss is the matrix factorization method. Consider the imaginary case where we have only four movies and three users, and we know all the ratings. That is, we have the following utility matrix:\n",
    "\n",
    "$$R = \\begin{bmatrix} 4 & 1 & 1 & 5\\\\ 5 & 5 & 2 & 1\\\\ 3 & 5 & 4 & 3\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "We can imagine that we can decompose this matrix into two other matrices, $U$ (for users) and $M$ (for movies), where \n",
    "\n",
    "$$U \\cdot M = R$$\n",
    "\n",
    "We can also imagine that we need to define the shape of $U$ and $M$. Let's say we want $U$ to be a $3\\times 3$ matrix and $M$ to be a $3\\times 4$ matrix.\n",
    "\n",
    "\n",
    "Through a bit of math that we won't discuss now, we can find two matrices which satisfy our demands. Their approximate values will be:\n",
    "\n",
    "\n",
    "$$U = \\begin{bmatrix} 0.029 & 2.898 & 0.46\\\\ 2.33 & 0.61 & 0.0\\\\ 0.561 & 0.0 &1.87\\end{bmatrix}$$\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "$$M = \\begin{bmatrix} 1.78 & 2.13 & 0.75 & 0.0\\\\ 1.36 & 0.0 & 0.37 & 1.63\\\\ 0.0 & 2.03 & 1.91 & 0.53\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "We've now decomposed the rating matrix into a matrix that tells us something about users and movies. The columns of the $U$ matrix and rows of the $M$ matrix tell us something about each user and movie, it tells us about some _latent_ (hidden) variables for each user and movie.\n",
    "\n",
    "To find out what the latent variables are, we need to know more about items being rated.\n",
    "\n",
    "Imagine our rating matrix was actually labeled like so:\n",
    "\n",
    "|User  | Finding Nemo |  Thor  | Thor: Ragnarok |  Finding Dory  |\n",
    "|------|--------------|--------|----------------|----------------|\n",
    "|Bob   |   4          | 1      |   1            | 5              |\n",
    "|Sally |   5          | 5      |   2            | 1              |\n",
    "|Shila |   3          | 5      |   4            | 3              |\n",
    "\n",
    "This means our user matrix is as follows:\n",
    "\n",
    "|      | Latent Variable1 |  Latent Variable2  | Latent Variable3|\n",
    "|------|------------------|--------------------|-----------------|\n",
    "|Bob   |   0.29           |         2.89       |   0.46          |\n",
    "|Sally |   2.33           | 0.61               |   0.0           |\n",
    "|Shila |   0.56           | 0.0                |   1.87          |\n",
    "\n",
    "\n",
    "and our movie matrix is as follows:\n",
    "\n",
    "|                 | Finding Nemo |  Thor  | Thor: Ragnarok |  Finding Dory  |\n",
    "|-----------------|--------------|--------|----------------|----------------|\n",
    "|Latent Variable1 |   1.78       | 2.13   |   0.75         | 0.0            |\n",
    "|Latent Variable2 |   1.36       | 0.0    |   0.37         | 1.63           |\n",
    "|Latent Variable3 |   0.0        | 2.03   |   1.91         | 0.53           |\n",
    "\n",
    "\n",
    "Latent variable 1 seems to be concerned with original movies vs sequels (Finding Nemo vs Finding Dory, and Thor vs Thor: Ragnarok). Latent variable 2 seems to be concerned with CG movies (or movies about marine life, we're not sure). Finally, latent variable 3 seems to be most active with Chris Hemsworth movies (or Marvel movies).\n",
    "\n",
    "\n",
    "Looking at our users, we can see that Bob is very high in his score of latent variable 2, he likes CG movies about marine life. Sally, on the other hand, is mostly concerned with originals vs sequels. Finally, Shila is interested in Marvel movies or Chris Hemsworth movies. Now we know something about each user and each movie that wasn't explicit in the data provided, we had to infer it by inspecting the movies and users.\n",
    "\n",
    "To be clear, we could programmatically find the values of the latent variables, but we had to perform manual inspection to understand what the variables represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funk Singular Value Decomposition (FunkSVD)\n",
    "\n",
    "Actually, we can't use traditional matrix factorization techniques on our rating matrix. Recall what our rating matrix looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's mostly filled with NaN values, and we can't impute them because that will throw off the mathematical methods. **So what do we do?**\n",
    "\n",
    "Well, we are going to use a method called [FunkSVD](https://sifter.org/~simon/journal/20061211.html) (which stands for Funk Singular Value Decomposition, invented by Simon Funk - won the Netflix competition). This method treats this as an optimization problem.\n",
    "\n",
    "We have our matrix with known values, and we want to find $x$ latent variables. We will try and find two matrices; $U$ and $M$ which get as close as possible to the values we DO know.\n",
    "\n",
    "This method is implemented in the `surprise` package ([more documentation here](https://surprise.readthedocs.io/en/stable/matrix_factorization.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise.reader import Reader\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD as FunkSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way FunkSVD in `surprise` works, it assigns each user with a value starting at 0. It also does so by the order it has seen users (it doesn't assume user will be recorded as numbers, we can have unique string IDs there and the algorithm will work just the same).\n",
    "\n",
    "Let's sort the dataframe so we have a slightly easier time making sense of the conversion FunkSVD does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df.sort_values(by=['user', 'movie'])\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load our dataframe as a special `Dataset` object from `surprise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = Dataset.load_from_df(original_df, Reader(rating_scale=(1, 5)))\n",
    "my_train_dataset = my_dataset.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we do is initialize the algorithm, specify the number of latent variables and iterations we'd like to use, and then let the algorithm run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_algorithm = FunkSVD(n_factors=10, \n",
    "                       n_epochs=100, \n",
    "                       lr_all=0.1,    # Learning rate for each epoch\n",
    "                       biased=False,  # This forces the algorithm to store all latent information in the matrices\n",
    "                       verbose=0)\n",
    "\n",
    "my_algorithm.fit(my_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user matrix is stored under the `SVD.pu` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = my_algorithm.pu\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the movie matrix is stored under the `SVD,qi` attribute, notice it is transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = my_algorithm.qi.T\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had more information about each item, we could begin exploring what the latent factors are by picking a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie = M[:, 0]\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.barh([f'Latent Var{i}' for i in range(1,len(first_movie)+1)], first_movie)\n",
    "plt.title(\"Latent Variable Composition of Movie #1\")\n",
    "plt.ylabel(\"Latent Variable\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what we _can_ do is now make new predictions. Let's explore our original movie rating data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the rating user 1 gave movie 2 (it's 3.0), so let's use this to demonstrate how we calculate ratings using these latent factors.\n",
    "\n",
    "First, we grab the user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_user_id = my_train_dataset.to_inner_uid(1) # find the inner representation of user 1\n",
    "user_profile = U[inner_user_id]\n",
    "user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.barh([f'Latent Var{i}' for i in range(1,len(first_movie)+1)], user_profile)\n",
    "plt.title(\"Latent Variable Profile of User #1\")\n",
    "plt.ylabel(\"Latent Variable\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we grab the movie profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_movie_id = my_train_dataset.to_inner_iid(2) # find the inner representation of item 1\n",
    "movie_profile = M[:, inner_movie_id]\n",
    "movie_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.barh([f'Latent Var{i}' for i in range(1,len(first_movie)+1)], movie_profile)\n",
    "plt.title(\"Latent Variable Profile of Movie #2\")\n",
    "plt.ylabel(\"Latent Variable\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expected rating of this movie by this user is the dot product of these two profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rating = np.dot(user_profile, movie_profile)\n",
    "expected_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, our ratings will get better and better as we use more latent variables. However, too many latent variables and we might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization Review\n",
    "\n",
    "The matrix factorization methods treat the rating prediction problem as an optimization problem. They try to optimize two matrices which combine to form the observed ratings. \n",
    "\n",
    "An added benefit the provide is creating profiles of user and movies that can be inspect to find underlying trends. Like all optimization methods we saw, these methods come with a set of hyperparameters to adjust:\n",
    "\n",
    "* **epochs**: Number of iterations the algorithm runs for. \n",
    "* **learning rate**: The speed at which the algorithm learns. Larger values give faster learning, but smaller values give more accurate learning. In `surprise`, you can adjust a global learning rate, or a learning rate per matrix.\n",
    "* **number of factors**: The number of latent factors the algorithm attempts to learn. More factors can give better results, but can also lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCHgnWriMpI6"
   },
   "source": [
    "### Recommender Systems Evaluation\n",
    "\n",
    "How do we evaluate the quality of our predictions and recommendations? Is there a standard metric used in practice? First we need to settle on what are we evaluating:\n",
    "- is it the score we fill into the user-item matrix?\n",
    "- is it the actual list of recommended items?\n",
    "- is it the utility of the recommended items (did it prompt user engagement)?\n",
    "\n",
    "To compare true and predicted scores, we often use the *root mean square error* (RMSE):\n",
    "\n",
    "$$\\sqrt{\\frac{1}{n}\\sum_{ij}(R_{ij}-\\hat R_{ij})^2}$$\n",
    "\n",
    "Here, $n$ stands for the number of scores and $\\hat R_{ij}$ is our prediction for the actual score $R_{ij}$. This seems like a reasonable measure, it's basically a regression measure, but is it any good? There are a few problems with it:\n",
    "\n",
    "1. **It can only measure the accuracy on user-item pairs we have data for.** What if our system makes a recommendation for a previously unseen item, which it really should be doing, how do we evaluate this? Are the reviews on seen items correlated with the reviews on unseen items?\n",
    "2. **Does an increase in performance (lower RMSE) imply an increase in revenue?** The end goal of the system is to increase user interaction and revenue, but do better score predictions mean more money? Intuitively it seems like suggesting products a user likes should increase money but there is no hard evidence backing this up.\n",
    "3. **Different users rank movies on different, often personal scales**: using the same 1-10 scale, some people never give too harsh scores while others submit only highly polarized reviews. This difference often hinders regression metrics to capture the real performance of a recommender system. This can be addressed though by scaling each user's recommendation before modeling.\n",
    "\n",
    "The `surprise` package implements RMSE and a few other evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# The surprise package doesn't allow you to test on the trainset we built\n",
    "my_train_dataset, my_test_dataset = train_test_split(my_dataset, test_size=0.5)\n",
    "\n",
    "predictions = my_algorithm.test(my_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "RMSE = accuracy.rmse(predictions, verbose=False)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RMSE, it is quite common to evaluate the *mean squared error* (MSE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{ij}(R_{ij}-\\hat R_{ij})^2$$\n",
    "\n",
    "or *mean absolute error* (MAE) \n",
    "\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{ij}|R_{ij}-\\hat R_{ij}|$$\n",
    "\n",
    "which penalize small versus large score deviations differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "MSE = accuracy.mse(predictions, verbose=False)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "MAE = accuracy.mae(predictions, verbose=False)\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with regression metrics is that they assume a numeric score, which is often not the case with recommender systems which can be built on various user-platform interactions. A useful **ranking-based metric** that overcomes this issue is the *Fraction of Concordant Pairs* (FCP).\n",
    "A *concordant pair of ratings* is composed of two pairs of true and predicted ratings $(r_{1}, \\hat{r}_1)$ and $(r_{2}, \\hat{r}_2)$ so that \n",
    "\n",
    "$$r_1 > r_2 \\text{ implies }\\hat r_1 > \\hat r_2$$\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "$$r_2 > r_1 \\text{ implies }\\hat r_2 > \\hat r_1.$$\n",
    "\n",
    "That is, the predicted and true ratings agree which of Movie 1 and Movie 2 was better (no matter their actual scores). Now, FCP is the ratio of concordant rating pairs to all rating pairs. Hence, the FCP ranges between the best score 1 and worst score 0. In many cases, a metric like this captures user intent better as it discounts for people using different scales for rating movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCP - Fraction of Concordant Pairs, the fraction of pairs whose relative ranking order is correct\n",
    "\n",
    "FCP = accuracy.fcp(predictions, verbose=False)\n",
    "print(FCP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of recommender systems are often **evaluated by usage** that is, how many of the predicted items prompt the user to interaction (clicking on a recommended movie). In turn, one can use classification metrics (accuracy, precision, recall) to see what percentage of our recommendations resulted in user interaction. Such an evaluation is only possible in an online setting where we can test our system with live users but could tell us more about the actual business impact of the recommendations.\n",
    "\n",
    "\n",
    "You can dive deep into various evaluation techniques and the many angles of measuring recommender system performance in [this article from Microsoft](http://www.bgu.ac.il/~shanigu/Publications/EvaluationMetrics.17.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (BONUS) Exercise 2\n",
    "\n",
    "Try and increase the number of latent factors and create a test plot with the evaluation metric of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the `FunkSVD` model with a varying number of factors and check the corresponding FCP score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_train_dataset, my_test_dataset = train_test_split(my_dataset, test_size=0.2)\n",
    "\n",
    "fcps = []\n",
    "factors = range(10, 201, 5)\n",
    "for n_factors in factors:    \n",
    "    my_algorithm = FunkSVD(n_factors=n_factors, \n",
    "                           n_epochs=50, \n",
    "                           lr_all=0.1,    # Learning rate for each epoch\n",
    "                           biased=False,  # This forces the algorithm to store all latent information in the matrices\n",
    "                           verbose=0)\n",
    "    \n",
    "    my_algorithm.fit(my_train_dataset)\n",
    "    predictions = my_algorithm.test(my_test_dataset)\n",
    "    \n",
    "    FCP = accuracy.fcp(predictions, verbose=False)\n",
    "    fcps.append(FCP)\n",
    "    print(n_factors, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(factors, fcps)\n",
    "plt.xlabel(\"Latent Factor Number\")\n",
    "plt.ylabel(\"Fraction of Concordant Pairs\")\n",
    "plt.title(\"Fraction for Concordant Pairs for Various Numbers of Latent Factors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some gain to increase the number of factors all the way up to 125 or so but there is a steep drop after 180 factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhKhPvFyMpIx"
   },
   "source": [
    "### Hybrid Methods\n",
    "\n",
    "Most recommendation systems in production are actually hybrid systems: these methods combine various models when making their decision averaging scores and ranks from multiple models. These models could range from collaborative filtering and matrix factorization to deep neural networks that predict scores. This is essentially running an ensemble method and one of the most famous code competitions, the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) was won by such an ensemble too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1000/1*rCK9VjrPgpHUvSNYw7qcuQ@2x.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrKInJcxMpI-"
   },
   "source": [
    "### Stale Recommendations and Time Sensitivity\n",
    "\n",
    "If we don't update our model frequently then we're going to end up with *stale recommendations*. What we recommended last month may need to be changed for the current month: a user might have read a lot of self-help books last year but hasn't recently. \n",
    "\n",
    "Streaming services use the most recent user interactions, a suddenly closed movie or skipped track, to reorder their playlists in real time. Since a complete retraining of models is often not feasible computationally, there are various techniques to update model predictions with recent context for fresh recommendations. *Incremental learning* can allow updates for CF and matrix factorization models to update their parameters as new data is recorded. One can also assign weights to historical ratings which allows newer ratings to effect more the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCA_qyF1MpJB"
   },
   "source": [
    "### Cold Start\n",
    "\n",
    "One major issue with collaborative filtering is how do we deal with a new user with no information about them. This is also a problem with no agreed upon solution, but many heuristic based solutions. Let's say we're Netflix and we need to make recommendations to a new user:\n",
    "\n",
    "1. We could provide them with some basic recommendations and after they view enough movies, we start to provide them with content and collaborative based recommendations.\n",
    "2. We could craft some predefined classes of new users. These could be hand crafted based on our knowledge, say action movie fans, comedy fans, etc... These classes would have some recommendations associated with them. Once we have a few data points for a new user we make recommendations based on these classes and after enough data points we start making recommendations based on collaborative or content models.\n",
    "3. These predefined classes for new users could be learned. We could preform unsupervised clustering of users to learn of \"natural\" groups. Once we have enough initial data points, we can assign the new user to their class as before and make recommendations based on this.\n",
    "4. We could have users fill out a simple survey and group them into initial classes based on this.\n",
    "\n",
    "All of these are reasonable approaches but it's hard to say which works the best, there is no hard science to this, only trying what seems reasonable.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:25px\"\"width: 50px\" src =\"https://drive.google.com/uc?export=view&id=14VoXUJftgptWtdNhtNYVm6cjVmEWpki1\" />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Recommender Systems.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "environment": {
   "kernel": "python38-kernel",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
